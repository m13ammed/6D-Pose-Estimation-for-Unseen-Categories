set_env_variables.cuda = True
set_env_variables.device = '0'
set_env_variables.checkpoint_interval = 1
set_env_variables.log_interval =  1
set_env_variables.BS =  8  #batch size
set_env_variables.num_workers = 12

set_env_variables.pretrained_model = '/data/unseen_object_data/6D-Pose-Estimation-for-Unseen-Categories/weights/weights.pt' #path for model to load (supported for training and eval) or None, Does not load optim params

set_env_variables.data_root = "/data/unseen_object_data/tmp/full_dataset_setup/" #path to the orignial datasests path


#base_object_dataset defines the dataset. use base_object_dataset to input parameters shared between all datasets
# additionally you can add scope/base_object_dataset to specify parameters for a specific datasets
# for example train_lm/base_object_dataset defines parametrs only for limemod dataset 
# you can define this for all datasets you want, however only datasets listed inside the prepare_train_datasets.datasets_list will be considered

base_object_dataset.mode = 'train_pbr' #train_pbr, train, test, or other BOP modes
base_object_dataset.num_samples = -1 #number of images to take
base_object_dataset.min_vis = 0.3 
base_object_dataset.cache_dir = '/data/unseen_object_data/tmp/cache_new' #where to save/load cache
base_object_dataset.LBO_pc = True #to generate LBO for point clouds

train_lm/base_object_dataset.render_data_name = "lm_1G" #dataset name (based on the folder name)
train_lm/base_object_dataset.num_samples = 200 
train_lm/base_object_dataset.obj_take = [1,2,3,4,14,10,6,12,9,15]  #obj ids to include in this datasets. Useful to exlude some noisy objects or make train test splits

train_hb/base_object_dataset.render_data_name = "hb"
train_hb/base_object_dataset.num_samples = 200 
train_hb/base_object_dataset.obj_take = [1,3,4,5,6,8,11,12,13,15,16,17,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33] 

train_ycbv/base_object_dataset.render_data_name = "ycbv_new"
train_ycbv/base_object_dataset.num_samples = 200 
train_ycbv/base_object_dataset.obj_take = [2,3,4,6,7,8,9,12] 

prepare_train_datasets.datasets_list = ([@train_hb/base_object_dataset, @train_ycbv/base_object_dataset]) #example for multidataset traning 

#prepare_train_datasets.datasets_list = ([@train_lm/base_object_dataset,@train_hb/base_object_dataset, @train_ycbv/base_object_dataset]) #example for multidataset traning 

#prepare_train_datasets.datasets_list = ([@train_lm/base_object_dataset]) #example for single dataset traning 

#eval mode supports only one dataset at the time

eval/base_object_dataset.render_data_name = "lm1k" # or lmo  (to replicate our expirements). lm1k is limemod but a different set of images used in training 
eval/base_object_dataset.num_samples = 200 
eval/base_object_dataset.obj_take = [5,6,8,12,11] #eval dataset only includes objects not used in the training lm.
eval/base_object_dataset.mode = 'train_pbr' #must set to test for lmo dataset
set_env_variables.save_results= '/data/unseen_object_data/tmp/results/results_lm_pbr_document' #path to save the results in. These files are then used to generate poses, if you dont want to save keep as None

yaml_read.path = "/data/unseen_object_data/6D-Pose-Estimation-for-Unseen-Categories/config/dpfm_orig.yaml" #path to yaml file containing configurations for the dpfm
DPFMNet.cfg = @yaml_read()

#DPFM loss parameters
DPFMLoss.w_fmap =  1
DPFMLoss.w_acc =  1
DPFMLoss.w_nce =  1
DPFMLoss.nce_t =  0.07 
DPFMLoss.nce_num_pairs =  512 


train_net.model = @DPFMNet()
RMSprop.lr = 0.0005 
train_net.optimizer = @RMSprop
train_net.criterion = @DPFMLoss()
train_net.decay_iter =  500
train_net.decay_factor = 0.1
train_net.epochs = 5000
train_net.logging_dir = "/data/unseen_object_data/tmp/logs"
train_net.comment = "training_linemod_trial_doc" #additioanl comment added for tensorboard folders

choose_fmap2pointmap_solver.solver = @spacial_filtering_fmap2pointmap # @spacial_filtering_fmap2pointmap  or @naive_fmap2pointmap

